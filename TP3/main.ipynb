{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Detectionsegmentation et estimation de la pose des humains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\python310\\lib\\site-packages (8.3.49)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\python310\\lib\\site-packages (from ultralytics) (1.26.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\python310\\lib\\site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\python310\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\python310\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\python310\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\python310\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\python310\\lib\\site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\python310\\lib\\site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\python310\\lib\\site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\python310\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\isio\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\python310\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\python310\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\python310\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\python310\\lib\\site-packages (from ultralytics) (2.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python310\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\isio\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "ERROR: No matching distribution found for cv2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: playsound==1.2.2 in c:\\python310\\lib\\site-packages (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install ultralytics\n",
    "! pip install numpy\n",
    "! pip install cv2\n",
    "! pip install torch\n",
    "! pip install playsound==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Manipulations de base avec YOLO11 pour la d√©tection dans une image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Isio\\Documents\\TraitementImage\\TP3\\img\\animal\\animal2.jpg: 448x640 1 bird, 100.0ms\n",
      "Speed: 2.9ms preprocess, 100.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[ 99, 146, 202],\n",
      "        [ 99, 146, 202],\n",
      "        [ 99, 146, 202],\n",
      "        ...,\n",
      "        [ 73,  99, 145],\n",
      "        [ 73,  99, 145],\n",
      "        [ 73,  99, 145]],\n",
      "\n",
      "       [[ 99, 146, 202],\n",
      "        [ 99, 146, 202],\n",
      "        [ 98, 145, 201],\n",
      "        ...,\n",
      "        [ 73,  99, 145],\n",
      "        [ 73,  99, 145],\n",
      "        [ 73,  99, 145]],\n",
      "\n",
      "       [[ 99, 146, 202],\n",
      "        [ 98, 145, 201],\n",
      "        [ 98, 145, 201],\n",
      "        ...,\n",
      "        [ 73,  99, 145],\n",
      "        [ 74, 100, 146],\n",
      "        [ 74, 100, 146]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 81, 113, 178],\n",
      "        [ 78, 110, 175],\n",
      "        [ 76, 108, 173],\n",
      "        ...,\n",
      "        [ 96, 140, 193],\n",
      "        [ 93, 138, 189],\n",
      "        [ 93, 138, 189]],\n",
      "\n",
      "       [[ 84, 119, 183],\n",
      "        [ 82, 117, 181],\n",
      "        [ 79, 114, 178],\n",
      "        ...,\n",
      "        [ 94, 138, 191],\n",
      "        [ 91, 136, 187],\n",
      "        [ 91, 136, 187]],\n",
      "\n",
      "       [[ 88, 123, 187],\n",
      "        [ 86, 121, 185],\n",
      "        [ 84, 119, 183],\n",
      "        ...,\n",
      "        [ 93, 137, 190],\n",
      "        [ 90, 135, 186],\n",
      "        [ 90, 135, 186]]], dtype=uint8)\n",
      "orig_shape: (3326, 4810)\n",
      "path: 'c:\\\\Users\\\\Isio\\\\Documents\\\\TraitementImage\\\\TP3\\\\img\\\\animal\\\\animal2.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.9211044311523438, 'inference': 99.99299049377441, 'postprocess': 1.0018348693847656}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# detect animal\n",
    "model = YOLO(\"models/yolo11n.pt\")\n",
    "results = model(\"img/animal/animal2.jpg\")\n",
    "\n",
    "# object segmentation \n",
    "# model = YOLO(\"yolo11n-seg.pt\")\n",
    "# results = model(\"img/segmentation.jpg\")\n",
    "\n",
    "# detect person pose\n",
    "# model = YOLO(\"yolo11n-pose.pt\")\n",
    "# results = model(\"img/pose/pose2.jpg\")\n",
    "\n",
    "# oriented detection\n",
    "# model = YOLO(\"yolo11n-obb.pt\")\n",
    "# results = model(\"img/orientedDetection.jpg\")\n",
    "\n",
    "# classification\n",
    "# model = YOLO(\"yolo11n-cls.pt\")\n",
    "# results = model(\"img/pose/pose3.webp\")\n",
    "\n",
    "print(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.dirname('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playsound==1.2.2\n",
      "  Using cached playsound-1.2.2-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Using cached playsound-1.2.2-py2.py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: playsound\n",
      "Successfully installed playsound-1.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install playsound==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Application de suivi avec YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 176.3ms\n",
      "Speed: 3.3ms preprocess, 176.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 173.0ms\n",
      "Speed: 2.0ms preprocess, 173.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 147.0ms\n",
      "Speed: 1.0ms preprocess, 147.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 143.0ms\n",
      "Speed: 1.0ms preprocess, 143.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 201.0ms\n",
      "Speed: 1.0ms preprocess, 201.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 150.0ms\n",
      "Speed: 1.0ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 149.0ms\n",
      "Speed: 1.0ms preprocess, 149.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 151.0ms\n",
      "Speed: 2.0ms preprocess, 151.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 154.8ms\n",
      "Speed: 1.0ms preprocess, 154.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 141.9ms\n",
      "Speed: 1.0ms preprocess, 141.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 144.0ms\n",
      "Speed: 2.0ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 143.0ms\n",
      "Speed: 1.0ms preprocess, 143.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 163.0ms\n",
      "Speed: 2.0ms preprocess, 163.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 146.0ms\n",
      "Speed: 1.0ms preprocess, 146.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 139.0ms\n",
      "Speed: 3.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 202.0ms\n",
      "Speed: 1.0ms preprocess, 202.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 144.0ms\n",
      "Speed: 2.0ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 150.2ms\n",
      "Speed: 1.6ms preprocess, 150.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 140.0ms\n",
      "Speed: 1.0ms preprocess, 140.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 148.0ms\n",
      "Speed: 1.0ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 141.0ms\n",
      "Speed: 1.0ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 151.0ms\n",
      "Speed: 2.0ms preprocess, 151.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 161.5ms\n",
      "Speed: 1.0ms preprocess, 161.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 145.1ms\n",
      "Speed: 2.0ms preprocess, 145.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 142.0ms\n",
      "Speed: 1.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 140.0ms\n",
      "Speed: 2.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 159.0ms\n",
      "Speed: 2.0ms preprocess, 159.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 160.0ms\n",
      "Speed: 2.0ms preprocess, 160.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 158.0ms\n",
      "Speed: 2.0ms preprocess, 158.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 162.0ms\n",
      "Speed: 1.0ms preprocess, 162.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 143.0ms\n",
      "Speed: 1.0ms preprocess, 143.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 296 for command:\n        open \"c:\\Users\\Isio\\Documents\\TraitementImage\\TP3\\alarm.wav\" alias playsound_0.41817289685129133\n    The specified file cannot be played on the specified MCI device.  The file may be corrupt, not in the correct format, or no file handler available for this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m     box_color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     83\u001b[0m     text_color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malarm.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjClassStr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrackingId\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassConf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m textBgLength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\playsound.py:35\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     34\u001b[0m alias \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaysound_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(random())\n\u001b[1;32m---> 35\u001b[0m \u001b[43mwinCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msound\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m alias\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m winCommand(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m'\u001b[39m, alias, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime format milliseconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m durationInMS \u001b[38;5;241m=\u001b[39m winCommand(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, alias, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\playsound.py:31\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     27\u001b[0m     windll\u001b[38;5;241m.\u001b[39mwinmm\u001b[38;5;241m.\u001b[39mmciGetErrorStringA(errorCode, errorBuffer, \u001b[38;5;241m254\u001b[39m)\n\u001b[0;32m     28\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode() \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     30\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 296 for command:\n        open \"c:\\Users\\Isio\\Documents\\TraitementImage\\TP3\\alarm.wav\" alias playsound_0.41817289685129133\n    The specified file cannot be played on the specified MCI device.  The file may be corrupt, not in the correct format, or no file handler available for this format."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "\n",
    "from time import time\n",
    "from playsound import playsound\n",
    "\n",
    "timeForAlarm = 5\n",
    "\n",
    "classesToDetect = {\n",
    "    # 39: 'bottle', \n",
    "    # 41: 'cup', \n",
    "    # 42: 'fork', \n",
    "    # 43: 'knife', \n",
    "    # 44: 'spoon', \n",
    "    # 45: 'bowl', \n",
    "    67: 'cell phone', \n",
    "    # 73: 'book', \n",
    "    # 76: 'scissors', \n",
    "    # 79: 'toothbrush',\n",
    "}\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "if (not cap.isOpened()):\n",
    "    print(\"Error while opening the camera.\")\n",
    "    exit(1)\n",
    "framerate = 60\n",
    "period = int(1000/framerate)\n",
    "    \n",
    "model = YOLO(model=\"models/yolo11s.pt\")\n",
    "\n",
    "detectionTime = time()\n",
    "detectedClass = -1\n",
    "detectedId = -1\n",
    "\n",
    "while (True):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if (not ret):\n",
    "        print(\"Error while fetching the image from the camera.\")\n",
    "        break\n",
    "\n",
    "    # treat the image\n",
    "    results = model.track(frame, classes=[_ for _ in classesToDetect.keys()], tracker=\"bytetrack.yaml\")\n",
    "\n",
    "    labels = results[0].names\n",
    "    \n",
    "    # no objets of interest detected\n",
    "    if len(results[0]) == 0:\n",
    "        detectedClass = -1\n",
    "        detectedId = -1\n",
    "    \n",
    "    for detectedObj in results[0]:\n",
    "        boxes = detectedObj.boxes\n",
    "\n",
    "        objClass = int(boxes.cls[0])\n",
    "        objClassStr = labels[objClass]\n",
    "        classConf = boxes.conf.tolist()[0]\n",
    "        \n",
    "        if boxes.id == None: continue\n",
    "        \n",
    "        trackingId = int(boxes.id.tolist()[0])\n",
    "\n",
    "        # detected object of interest, mark the first one for tracking purpose\n",
    "        if detectedClass == -1:\n",
    "            detectionTime = time()\n",
    "            detectedClass = objClass\n",
    "            detectedId = trackingId\n",
    "\n",
    "        # bounding box coordinates \n",
    "        x1, y1, x2, y2 = [int(i) for i in boxes.xyxy[0].tolist()]\n",
    "        \n",
    "        box_color = (255, 255, 255)\n",
    "        text_color = (0, 0, 0)\n",
    "        \n",
    "        # object detected for long enough, play sound and switch bounding box and label color\n",
    "        timeElapsed = time() - detectionTime     \n",
    "        if timeElapsed > timeForAlarm: \n",
    "            box_color = (0, 0, 255)\n",
    "            text_color = (255, 255, 255)\n",
    "            \n",
    "            # playsound not working\n",
    "            # playsound(f'{os.path.abspath(\"./\")}\\\\{\"alarm.wav\"}', block=False)\n",
    "            \n",
    "        label = f\"{objClassStr} (id: {trackingId}): {classConf:.2f}\"\n",
    "        textBgLength = len(label)*8 + 20\n",
    "        \n",
    "        if trackingId == detectedId:\n",
    "            # bounding box\n",
    "            frame = cv.rectangle(frame, (x1, y1), (x2, y2), box_color, 3)\n",
    "            # background for label\n",
    "            frame = cv.rectangle(frame, (x1, y1), (x1+textBgLength, y1-30), box_color, -1)\n",
    "            # label with class, class index and conf\n",
    "            frame = cv.putText(frame, label, (x1+5, y1-10), cv.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2)\n",
    "        \n",
    "    cv.imshow(\"Camera\", frame)\n",
    "    \n",
    "    if (cv.waitKey(period) == ord('q')):\n",
    "        break        \n",
    "    \n",
    "cv.destroyAllWindows()\n",
    "cap.release()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
